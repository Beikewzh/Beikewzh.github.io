---
title: "Forward-Backward Knowledge Distillation for Continual Clustering"
date: 2030-01-01
selected: false
status: "submitted"
pub: "Submitted to WACV 2026"
pub_last:
pub_date: ""
abstract: >-
  We introduce Unsupervised Continual Clustering (UCC) and propose Forward-Backward Knowledge Distillation for Continual Clustering (FBCC) to address catastrophic forgetting in unsupervised continual learning. FBCC leverages a teacher-student distillation framework with both forward and backward knowledge transfer to enhance memory efficiency and clustering performance. Experiments show that FBCC outperforms existing methods on continual clustering tasks, marking a significant advance for unsupervised continual learning.
cover: /assets/images/covers/FBCC_cover.png
authors:
- Mohammadreza Sadeghi
- Zihan Wang
- Narges Armanfard
links:
  Paper: https://arxiv.org/abs/2405.19234
---
